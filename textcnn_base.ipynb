{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用外部词向量\n",
    "#coding=utf-8\n",
    "import pandas as pd\n",
    "# df_train = pd.read_csv(\"/ceph/11122/tianchi/train_set.csv\", sep=\"\\t\")\n",
    "# df_test = pd.read_csv(\"/ceph/11122/tianchi/test_a.csv\", sep=\"\\t\")\n",
    "df_train = pd.read_csv(r\"D:\\ANewStart\\dataset\\天池新闻文本分类数据集\\train_set.csv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(r\"D:\\ANewStart\\dataset\\天池新闻文本分类数据集test_a.csv\", sep=\"\\t\")\n",
    "df_train[\"text\"]=df_train[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))\n",
    "df_test[\"text\"]=df_test[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))\n",
    "SEED=2020\n",
    "num_classes = 14\n",
    "vocabulary_size = 7600\n",
    "maxlen=1024\n",
    "batch_size = 16\n",
    "embedding_dim = 256\n",
    "num_filters = 512\n",
    "filter_sizes = [3,4,5]\n",
    "drop = 0.5\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=SEED)\n",
    "def load_data(df):\n",
    "    D = list()\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        D.append((text, int(label)))\n",
    "    return D\n",
    "train_data = load_data(df_train)\n",
    "valid_data = load_data(df_valid)\n",
    "\n",
    "from bert4keras.snippets import DataGenerator, sequence_padding\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size=32, buffer_size=None, random=False):\n",
    "        super().__init__(data, batch_size, buffer_size)\n",
    "        self.random = random\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_labels = [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids = text[:maxlen] if len(text) > maxlen else text + (maxlen - len(text)) * [0]\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids], batch_labels\n",
    "                batch_token_ids, batch_labels = [], []\n",
    "\n",
    "    def forfit(self):\n",
    "        while True:\n",
    "            for d in self.__iter__(self.random):\n",
    "                yield d\n",
    "\n",
    "\n",
    "train_generator = data_generator(train_data, batch_size, random=True)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# 输入\n",
    "inputs = Input(shape=(maxlen,), dtype=\"int32\")\n",
    "\n",
    "# 嵌入层\n",
    "embedding = Embedding(\n",
    "    input_dim=vocabulary_size, # 词典size\n",
    "    output_dim=embedding_dim, # 词向量size\n",
    "    input_length=maxlen # 输入size\n",
    ")(inputs) # 输入\n",
    "reshape = Reshape((maxlen, embedding_dim, 1))(embedding) # 加一个维度\n",
    "\n",
    "# 卷积层\n",
    "conv_0 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[0], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_1 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[1], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_2 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[2], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "# 池化\n",
    "maxpool_0 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[0]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_0)\n",
    "maxpool_1 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[1]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_1)\n",
    "maxpool_2 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[2]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_2)\n",
    "\n",
    "# 输出层\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0,maxpool_1,maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=num_classes, activation=\"softmax\")(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Evaluator(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0\n",
    "    def evaluate(self):\n",
    "        y_true, y_pred = list(), list()\n",
    "        for x, y in valid_generator:\n",
    "            y_true.append(y)\n",
    "            y_pred.append(self.model.predict(x).argmax(axis=1))\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        return\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1>self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs[\"val_f1\"] = val_f1\n",
    "        print(f\"val_f1:{val_f1:.5f}, best_val_f1:{self.best_val_f1:.5f}\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluator(),\n",
    "    EarlyStopping(\n",
    "        monitor = \"val_loss\",\n",
    "        patience = 1,\n",
    "        verbose = 1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        \"textcnn_best_model.weights\",\n",
    "        monitor=\"val_f1\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode=\"max\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_generator.forfit(),\n",
    "    validation_steps=len(valid_generator)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
