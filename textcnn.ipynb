{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻文本分类使用textcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==1.14.0 bert4keras sklearn pandas numpy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bert4keras.snippets import DataGenerator, sequence_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 导入数据(已经被转化成index)\n",
    "# df_train = pd.read_csv(r\"D:\\ANewStart\\dataset\\天池新闻文本分类数据集\\train_set.csv\", sep=\"\\t\")\n",
    "# df_test = pd.read_csv(r\"D:\\ANewStart\\dataset\\天池新闻文本分类数据集\\test_a.csv\", sep=\"\\t\")\n",
    "# df_train[\"text\"]=df_train[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))\n",
    "# df_test[\"text\"]=df_test[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入词向量(7000小词典，百度云)\n",
    "def w2v_model_preprocessing():\n",
    "    # 导入模型\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r\"D:\\ANewStart\\dataset\\腾讯词向量\\70000-small.txt\", binary=False)\n",
    "    word2idx = {\"_PAD\": 0} # 词->index\n",
    "    vocab_list = [(k, w2v_model.wv[k]) for k, v in w2v_model.wv.vocab.items()]\n",
    "    # 存储所有word2vec中所有词向量的数组，其中第一个全为0用于padding\n",
    "    embeddings_matrix = np.zeros((len(w2v_model.wv.vocab.items())+1, w2v_model.vector_size))\n",
    "    # 填充字典和矩阵\n",
    "    for i in range(len(vocab_list)):\n",
    "        word = vocab_list[i][0]\n",
    "        word2idx[word] = i+1\n",
    "        embeddings_matrix[i+1] = vocab_list[i][1]\n",
    "    return w2v_model, word2idx, embeddings_matrix\n",
    "w2v_model, word2idx, embeddings_matrix = w2v_model_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词转index\n",
    "def get_words_index(data, word_index):\n",
    "    new_txt = []\n",
    "    for word in data:\n",
    "        try:\n",
    "            new_txt.append(word_index[word])\n",
    "        except:\n",
    "            new_txt.append(0) # 不存在填充0\n",
    "    return new_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已经处理好的中文新闻语料\n",
    "df = pd.read_csv(r\"D:\\ANewStart\\dataset\\复旦新闻语料\\article_features_train_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除含nan的行\n",
    "df = df.dropna(axis=0, how=\"any\") # any/all 任何/全部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数量\n",
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文label转索引\n",
    "lable_index = {'艺术':0, '文学':1, '哲学':2, '通信':3, '能源':4, '历史':5,\n",
    "               '矿藏':6, '空间':7, '教育':8, '交通':9, '计算机':10, '环境':11,\n",
    "               '电子':12, '农业':13, '体育':14, '时政':15, '医疗':16, '经济':17, '法律':18}\n",
    "df[\"label\"] =df[\"label\"].apply(lambda x:lable_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文语料转index\n",
    "df[\"text\"] = df[\"words\"].apply(lambda x:get_words_index(x.split(), word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 索引-中文字典\n",
    "# dict(zip(word2idx.values(), word2idx.keys()))[9156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"text\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "SEED=2020\n",
    "num_classes = 19\n",
    "vocabulary_size = 7000\n",
    "maxlen=128\n",
    "batch_size = 1\n",
    "# embedding_dim = 256\n",
    "embedding_dim = 200 # 匹配词向量 改成200 自己初始化训练就不需要匹配\n",
    "num_filters = 512\n",
    "filter_sizes = [3,4,5]\n",
    "drop = 0.5\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分、加载、组织数据\n",
    "df_train, df_valid = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "def load_data(df):\n",
    "    D = list()\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        D.append((text, int(label)))\n",
    "    return D\n",
    "train_data = load_data(df_train)\n",
    "valid_data = load_data(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据生成器\n",
    "class data_generator(DataGenerator):\n",
    "    def __init__(self, data, batch_size=32, buffer_size=None, random=False):\n",
    "        super().__init__(data, batch_size, buffer_size)\n",
    "        self.random = random\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_labels = [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids = text[:maxlen] if len(text) > maxlen else text + (maxlen - len(text)) * [0]\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids], batch_labels # 输出一个batch或者最后剩余\n",
    "                batch_token_ids, batch_labels = [], []\n",
    "    def forfit(self):\n",
    "        while True:\n",
    "            for d in self.__iter__(self.random):\n",
    "                yield d # 一个一个输出\n",
    "train_generator = data_generator(train_data, batch_size, random=True)\n",
    "valid_generator = data_generator(valid_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.3.1 in d:\\python36\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\python36\\lib\\site-packages (from keras==2.3.1) (1.16.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in d:\\python36\\lib\\site-packages (from keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in d:\\python36\\lib\\site-packages (from keras==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: h5py in d:\\python36\\lib\\site-packages (from keras==2.3.1) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in d:\\python36\\lib\\site-packages (from keras==2.3.1) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in d:\\python36\\lib\\site-packages (from keras==2.3.1) (5.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\python36\\lib\\site-packages (from keras==2.3.1) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==1.14.0 in d:\\python36\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.16.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.22.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (0.26.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in d:\\python36\\lib\\site-packages (from tensorflow-gpu==1.14.0) (0.8.0)\n",
      "Requirement already satisfied: setuptools in d:\\python36\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (41.6.0)\n",
      "Requirement already satisfied: h5py in d:\\python36\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\python36\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\python36\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (0.15.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 12:11:37.356636  2368 deprecation_wrapper.py:119] From d:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "# 输入\n",
    "inputs = Input(shape=(maxlen,), dtype=\"int32\")\n",
    "\n",
    "# 嵌入层\n",
    "embedding = Embedding(\n",
    "#     input_dim=vocabulary_size, # 词典size\n",
    "    input_dim=len(embeddings_matrix),\n",
    "    output_dim=embedding_dim, # 词向量size\n",
    "    input_length=maxlen, # 输入size\n",
    "    weights=[embeddings_matrix], # 这里引入外部词向量\n",
    "    trainable=False # 外部词向量不变\n",
    ")(inputs) # 输入\n",
    "reshape = Reshape((maxlen, embedding_dim, 1))(embedding) # 加一个维度\n",
    "\n",
    "# 卷积层\n",
    "conv_0 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[0], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_1 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[1], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_2 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[2], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "# 池化\n",
    "maxpool_0 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[0]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_0)\n",
    "maxpool_1 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[1]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_1)\n",
    "maxpool_2 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[2]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_2)\n",
    "\n",
    "# 输出层\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0,maxpool_1,maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=num_classes, activation=\"softmax\")(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回调\n",
    "class Evaluator(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0 # f1\n",
    "    def evaluate(self):\n",
    "        y_true, y_pred = list(), list()\n",
    "        for x, y in valid_generator: #\n",
    "            y_true.append(y)\n",
    "            y_pred.append(self.model.predict(x).argmax(axis=1)) # 取预测值最高\n",
    "        y_true = np.concatenate(y_true) # 拼接一个batch\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\") # 计算f1\n",
    "        return f1\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1>self.best_val_f1: # best f1 更新\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs[\"val_f1\"] = val_f1\n",
    "        print(f\"val_f1:{val_f1:.5f}, best_val_f1:{self.best_val_f1:.5f}\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluator(),\n",
    "    EarlyStopping( # 早停\n",
    "        monitor = \"val_loss\", # 监控指标\n",
    "        patience = 1, # 容忍epoch\n",
    "        verbose = 1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        \"best_model.weights\", # 参数\n",
    "        monitor=\"val_f1\", # 监控指标\n",
    "        save_weights_only=True, # 只保存参数\n",
    "        save_best_only=True, # 清除历史\n",
    "        verbose=1,\n",
    "        mode=\"max\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_generator.forfit(),\n",
    "    validation_steps=len(valid_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
