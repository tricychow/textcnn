{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻文本分类使用textcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==1.14.0 bert4keras sklearn pandas numpy gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "from bert4keras.snippets import DataGenerator, sequence_padding\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "# df_train = pd.read_csv(\"/ceph/11122/tianchi/train_set.csv\", sep=\"\\t\")\n",
    "# df_test = pd.read_csv(\"/ceph/11122/tianchi/test_a.csv\", sep=\"\\t\")\n",
    "# df_train[\"text\"]=df_train[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))\n",
    "# df_test[\"text\"]=df_test[\"text\"].apply(lambda x:list(map(lambda y:int(y), x.split())))\n",
    "\n",
    "\n",
    "wordkeyvector = \"/ceph/11122/txkeyvector/70000-small.txt\"\n",
    "news = \"/ceph/11122/txkeyvector/article_features_train_raw.csv\"\n",
    "\n",
    "def w2v_model_preprocessing():\n",
    "    # 导入模型\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(wordkeyvector, binary=False)\n",
    "    word2idx = {\"_PAD\": 0} # 词->index\n",
    "    vocab_list = [(k, w2v_model.wv[k]) for k, v in w2v_model.wv.vocab.items()]\n",
    "    # 存储所有word2vec中所有词向量的数组，其中第一个全为0用于padding\n",
    "    embeddings_matrix = np.zeros((len(w2v_model.wv.vocab.items())+1, w2v_model.vector_size))\n",
    "    # 填充字典和矩阵\n",
    "    for i in range(len(vocab_list)):\n",
    "        word = vocab_list[i][0]\n",
    "        word2idx[word] = i+1\n",
    "        embeddings_matrix[i+1] = vocab_list[i][1]\n",
    "    return w2v_model, word2idx, embeddings_matrix\n",
    "w2v_model, word2idx, embeddings_matrix = w2v_model_preprocessing()\n",
    "\n",
    "def get_words_index(data, word_index):\n",
    "    new_txt = []\n",
    "    for word in data:\n",
    "        try:\n",
    "            new_txt.append(word_index[word])\n",
    "        except:\n",
    "            new_txt.append(0)\n",
    "    return new_txt\n",
    "\n",
    "df = pd.read_csv(news)\n",
    "df = df.dropna(axis=0, how=\"any\")\n",
    "\n",
    "lable_index = {'艺术':0, '文学':1, '哲学':2, '通信':3, '能源':4, '历史':5,\n",
    "               '矿藏':6, '空间':7, '教育':8, '交通':9, '计算机':10, '环境':11,\n",
    "               '电子':12, '农业':13, '体育':14, '时政':15, '医疗':16, '经济':17, '法律':18}\n",
    "df[\"label\"] =df[\"label\"].apply(lambda x:lable_index[x])\n",
    "\n",
    "df[\"text\"] = df[\"words\"].apply(lambda x:get_words_index(x.split(), word2idx))\n",
    "\n",
    "# dict(zip(word2idx.values(), word2idx.keys()))[9156]\n",
    "# df[\"words\"][:10]\n",
    "# df[\"text\"][:10]\n",
    "\n",
    "SEED=2020\n",
    "num_classes = 19\n",
    "vocabulary_size = 7000\n",
    "maxlen=1024\n",
    "batch_size = 1024\n",
    "# embedding_dim = 256\n",
    "embedding_dim = 200 # 匹配词向量 改成200\n",
    "num_filters = 512\n",
    "filter_sizes = [3,4,5]\n",
    "drop = 0.5\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_valid = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "def load_data(df):\n",
    "    D = list()\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        D.append((text, int(label)))\n",
    "    return D\n",
    "train_data = load_data(df_train)\n",
    "valid_data = load_data(df_valid)\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size=32, buffer_size=None, random=False):\n",
    "        super().__init__(data, batch_size, buffer_size)\n",
    "        self.random = random\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_labels = [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids = text[:maxlen] if len(text) > maxlen else text + (maxlen - len(text)) * [0]\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids], batch_labels\n",
    "                batch_token_ids, batch_labels = [], []\n",
    "    def forfit(self):\n",
    "        while True:\n",
    "            for d in self.__iter__(self.random):\n",
    "                yield d\n",
    "train_generator = data_generator(train_data, batch_size, random=True)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n",
    "\n",
    "# 输入\n",
    "inputs = Input(shape=(maxlen,), dtype=\"int32\")\n",
    "\n",
    "# 嵌入层\n",
    "embedding = Embedding(\n",
    "#     input_dim=vocabulary_size, # 词典size\n",
    "    input_dim=len(embeddings_matrix), # 输入因为引入词向量所以这里输入维度改变成词向量size\n",
    "    output_dim=embedding_dim, # 词向量size\n",
    "    input_length=maxlen, # 输入size\n",
    "    weights=[embeddings_matrix],  # 为了引入词向量\n",
    "    trainable=False # 引入词向量不训练\n",
    ")(inputs) # 输入\n",
    "reshape = Reshape((maxlen, embedding_dim, 1))(embedding) # 加一个维度\n",
    "\n",
    "# 卷积层\n",
    "conv_0 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[0], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_1 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[1], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "conv_2 = Conv2D(\n",
    "    num_filters, # 输出size\n",
    "    kernel_size=(filter_sizes[2], embedding_dim), # 卷积核宽高，宽=卷积核数，高=词向量size\n",
    "    padding=\"valid\",\n",
    "    kernel_initializer=\"normal\",\n",
    "    activation=\"relu\"\n",
    ")(reshape)\n",
    "# 池化\n",
    "maxpool_0 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[0]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_0)\n",
    "maxpool_1 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[1]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_1)\n",
    "maxpool_2 = MaxPool2D(\n",
    "    pool_size = (maxlen-filter_sizes[2]+1, 1),\n",
    "    strides = (1,1),\n",
    "    padding=\"valid\"\n",
    ")(conv_2)\n",
    "\n",
    "# 输出层\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0,maxpool_1,maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=num_classes, activation=\"softmax\")(dropout)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=lr),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "class Evaluator(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0\n",
    "    def evaluate(self):\n",
    "        y_true, y_pred = list(), list()\n",
    "        for x, y in valid_generator:\n",
    "            y_true.append(y)\n",
    "            y_pred.append(self.model.predict(x).argmax(axis=1))\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        return f1\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1>self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs[\"val_f1\"] = val_f1\n",
    "        print(f\"val_f1:{val_f1:.5f}, best_val_f1:{self.best_val_f1:.5f}\")\n",
    "\n",
    "callbacks = [\n",
    "    Evaluator(),\n",
    "    EarlyStopping(\n",
    "        monitor = \"val_loss\",\n",
    "        patience = 1,\n",
    "        verbose = 1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        \"best_model.weights\",\n",
    "        monitor=\"val_f1\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        mode=\"max\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "model.load_weights(\"best_model.weights\")\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_generator.forfit(),\n",
    "    validation_steps=len(valid_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测测试集并输出csv\n",
    "df_test[\"label\"] = 0\n",
    "test_data = load_data(df_test)\n",
    "test_generator = data_generator(test_data, batch_size)\n",
    "result = model.predict_generator(test_generator.forfit(), steps=len(test_generator))\n",
    "result = result.argmax(axis=1)\n",
    "df_test[\"label\"] = result\n",
    "# df_test.to_csv(\"submission.csv\", index=False, columns=[\"label\"])\n",
    "df_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "model.load_weights(\"best_model.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测随机数据\n",
    "data = np.random.randint(0, 6900, size=1024).tolist()\n",
    "print(model.predict(np.array(example)))\n",
    "print(np.argmax(model.predict(np.array(example))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
